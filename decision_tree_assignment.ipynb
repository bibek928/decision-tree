{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c8163a2",
   "metadata": {},
   "source": [
    "# Decision Tree Assignment Solutions\n",
    "\n",
    "Generated by ChatGPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3002b9df",
   "metadata": {},
   "source": [
    "## Question 1: What is a Decision Tree, and how does it work in the context of classification?\n",
    "\n",
    "A decision tree is a flowchart-like model used for classification (and regression) that recursively splits the feature space into regions based on feature values. In classification, each internal node tests a feature, branches represent possible values/ranges, and leaf nodes assign class labels. The tree is built by selecting splits that maximize purity (e.g., information gain or reduction in impurity), leading to hierarchical decision rules that classify input samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c20ddf",
   "metadata": {},
   "source": [
    "## Question 2: Explain the concepts of Gini Impurity and Entropy as impurity measures. How do they impact the splits in a Decision Tree?\n",
    "\n",
    "- **Gini Impurity** measures the probability of incorrectly classifying a randomly chosen element if it were labeled according to the class distribution in the node. Formula: 1 - Σ p_i^2. Lower is purer.\n",
    "- **Entropy** comes from information theory and quantifies the amount of disorder: -Σ p_i log2(p_i). Lower entropy means more homogenous.\n",
    "\n",
    "When choosing splits, the algorithm evaluates how much a split reduces impurity (Gini or Entropy). Splits that yield the largest reduction (highest information gain for entropy) are preferred, leading to more informative partitions of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3199c7e0",
   "metadata": {},
   "source": [
    "## Question 3: What is the difference between Pre-Pruning and Post-Pruning in Decision Trees? Give one practical advantage of using each.\n",
    "\n",
    "- **Pre-Pruning**: Stops tree growth early based on criteria (e.g., max depth, minimum samples per leaf) to prevent overfitting. *Advantage:* faster training and simpler model by restricting complexity upfront.\n",
    "- **Post-Pruning**: Grows a full tree then removes branches that do not improve generalization using validation data. *Advantage:* can recover from suboptimal early splits and often yields more accurate final trees because it considers the full structure before trimming."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60aa77b2",
   "metadata": {},
   "source": [
    "## Question 4: What is Information Gain in Decision Trees, and why is it important for choosing the best split?\n",
    "\n",
    "Information Gain is the reduction in impurity (usually entropy) achieved by partitioning the data on a feature. It is computed as the difference between the parent node's impurity and the weighted average impurity of the child nodes after the split. The split with the highest information gain is chosen because it most effectively separates the classes, leading to purer subsets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b165f5bd",
   "metadata": {},
   "source": [
    "## Question 5: What are some common real-world applications of Decision Trees, and what are their main advantages and limitations?\n",
    "\n",
    "**Applications:**\n",
    "- Medical diagnosis (disease prediction)\n",
    "- Credit scoring and loan approval\n",
    "- Customer churn prediction\n",
    "- Fraud detection\n",
    "- Marketing segmentation\n",
    "\n",
    "**Advantages:**\n",
    "- Easy to interpret and visualize\n",
    "- Handles both numerical and categorical data\n",
    "- Requires little data preprocessing\n",
    "- Can capture nonlinear relationships\n",
    "\n",
    "**Limitations:**\n",
    "- Prone to overfitting if unrestricted\n",
    "- Unstable (small changes in data can lead to different trees)\n",
    "- Biased toward features with more levels\n",
    "- Less accurate than ensemble methods like Random Forest or Gradient Boosting in many cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8aae514",
   "metadata": {},
   "source": [
    "## Question 6: Load Iris, train Decision Tree Classifier (Gini), print accuracy and feature importances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4997ec7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "iris = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)\n",
    "clf_gini = DecisionTreeClassifier(criterion=\"gini\", random_state=42)\n",
    "clf_gini.fit(X_train, y_train)\n",
    "y_pred = clf_gini.predict(X_test)\n",
    "print(\"Accuracy (Gini):\", accuracy_score(y_test, y_pred))\n",
    "print(\"Feature importances:\", clf_gini.feature_importances_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df40cdd9",
   "metadata": {},
   "source": [
    "## Question 7: Compare Decision Tree with max_depth=3 vs fully-grown tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfffc417",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "clf_depth3 = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "clf_depth3.fit(X_train, y_train)\n",
    "acc_depth3 = accuracy_score(y_test, clf_depth3.predict(X_test))\n",
    "acc_full = accuracy_score(y_test, clf_gini.predict(X_test))\n",
    "print(\"Accuracy with max_depth=3:\", acc_depth3)\n",
    "print(\"Accuracy fully-grown:\", acc_full)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9fcd916",
   "metadata": {},
   "source": [
    "## Question 8: Load synthetic regression dataset (stand-in for Boston), train Decision Tree Regressor, print MSE and feature importances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02691461",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_reg, y_reg = make_regression(n_samples=506, n_features=13, noise=0.5, random_state=42)\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X_reg, y_reg, test_size=0.2, random_state=42)\n",
    "reg = DecisionTreeRegressor(random_state=42)\n",
    "reg.fit(X_train_reg, y_train_reg)\n",
    "y_pred_reg = reg.predict(X_test_reg)\n",
    "print(\"MSE:\", mean_squared_error(y_test_reg, y_pred_reg))\n",
    "print(\"Feature importances:\", reg.feature_importances_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5cf2e9",
   "metadata": {},
   "source": [
    "## Question 9: Tune max_depth and min_samples_split using GridSearchCV on Iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d4162d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "param_grid = {'max_depth': [2, 3, 4, 5, None], 'min_samples_split': [2, 4, 6, 8]}\n",
    "grid_search = GridSearchCV(DecisionTreeClassifier(random_state=42), param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best CV score:\", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63073a5b",
   "metadata": {},
   "source": [
    "## Question 10: Healthcare disease prediction pipeline\n",
    "\n",
    "**1. Handle missing values:**\n",
    "- Analyze missingness pattern (e.g., missing completely at random vs not).\n",
    "- Impute numerical features (mean/median or model-based) and categorical (most frequent or new category). Use techniques like KNN imputation if appropriate.\n",
    "\n",
    "**2. Encode categorical features:**\n",
    "- Use one-hot encoding for nominal categories or ordinal encoding if there is intrinsic order. For high-cardinality, consider target encoding with regularization.\n",
    "\n",
    "**3. Train Decision Tree model:**\n",
    "- Split data into train/validation/test.\n",
    "- Scale if needed (trees don't require scaling).\n",
    "- Initialize DecisionTreeClassifier; choose criterion (gini/entropy) based on interpretability preferences.\n",
    "\n",
    "**4. Tune hyperparameters:**\n",
    "- Use GridSearchCV or RandomizedSearchCV over parameters like max_depth, min_samples_split, min_samples_leaf, and class_weight to handle imbalance.\n",
    "- Use cross-validation and use a validation set to prevent overfitting.\n",
    "\n",
    "**5. Evaluate performance:**\n",
    "- Metrics: accuracy, precision, recall, F1-score, ROC AUC for binary disease prediction.\n",
    "- Use confusion matrix to understand types of errors.\n",
    "- Calibrate probabilities if needed.\n",
    "- Perform feature importance analysis and SHAP explanations for interpretability.\n",
    "\n",
    "**Business value:**\n",
    "- Early detection leading to timely treatment, reducing costs and improving outcomes.\n",
    "- Risk stratification to allocate resources efficiently.\n",
    "- Personalized care recommendations.\n",
    "- Reduces unnecessary tests by flagging high-risk patients."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
